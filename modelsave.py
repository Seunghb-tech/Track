# -*- coding: utf-8 -*-
"""modelSave.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjV4PISLPzXwe0M57g7F4t32xtFsirso
"""

# 모델 저장과 복원
# 훈련하는 도중이나 훈련이 끝난 후에 모델을 저장할 수 있다. 모델을 중지된 지점부터 다시 훈련할 수 있다. 
# 또 모델을 저장하면 다른 사람에게 공유할 수 있고 작업을 재현할 수 있다. 

# 연구한 모델과 기법을 공개할 때 많은 머신 러닝 기술자들이 다음과 같은 것들을 제공한다.
## 모델을 만드는 코드
## 모델의 훈련된 가중치 또는 파라미터

# 이런 데이터를 공유하면 다른 사람들이 모델의 작동 방식을 이해하고 새로운 데이터로 모델을 실험하는데 도움이 된다.

# 저장 방식. 사용하는 API에 따라서 여러가지 방법으로 텐서플로 모델을 저장할 수 있다. 
# 이 문서는 텐서플로 모델을 만들고 훈련하기 위한 고수준 API인 tf.keras를 사용
# !pip install -q h5py pyyaml

from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals

import os

import tensorflow as tf
from tensorflow import keras

tf.__version__

# MNIST 데이터셋으로 모델을 훈련하여 가중치를 저장하는 예제를 만들어 보자. 
# 모델 실행 속도를 빠르게 하기 위해 샘플에서 처음 1,000개만 사용
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

# 간단한 Sequential 모델을 반환
def create_model():
  model = tf.keras.models.Sequential([
    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation=tf.keras.activations.softmax)
  ])

  model.compile(optimizer=tf.keras.optimizers.Adam(),
                loss=tf.keras.losses.sparse_categorical_crossentropy,
                metrics=['accuracy'])

  return model


# 모델 객체를 만든다
model = create_model()
model.summary()

# 훈련 중간과 훈련 마지막에 checkpoint를 자동으로 저장하도록 하는 것이 많이 사용하는 방법
# 다시 훈련하지 않고 모델을 재사용하거나 훈련 과정이 중지된 경우 이어서 훈련을 진행할 수 있다.

# tf.keras.callbacks.ModelCheckpoint은 이런 작업을 수행하는 콜백(callback). 여러가지 매개변수를 제공

checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# 체크포인트 콜백 만들기
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)

model = create_model()

model.fit(train_images, train_labels,  epochs = 10, validation_data = (test_images,test_labels),
          callbacks = [cp_callback])  # 훈련 단계에 콜백을 전달

# 옵티마이저의 상태를 저장하는 것과 관련되어 경고가 발생할 수 있다. 이 경고는 이전 사용 방식을 권장하지 않기 위함이며 무시해도 좋다.

# !ls {checkpoint_dir}

# 훈련하지 않은 새로운 모델을 만들어 가중치를 공유해 보자. 가중치만 복원할 땐 원본 모델과 동일한 구조로 모델을 만들어야 한다.

# 훈련하지 않은 새 모델을 만들고 테스트 세트에서 평가해 보자. 훈련되지 않은 모델의 성능은 무작위로 선택하는 정도의 수준이다(~10% 정확도)

model = create_model()

loss, acc = model.evaluate(test_images, test_labels)
print("훈련되지 않은 모델의 정확도: {:5.2f}%".format(100*acc))

# 체크포인트에서 가중치를 로드하고 다시 평가해 보자.
model.load_weights(checkpoint_path)
loss,acc = model.evaluate(test_images, test_labels)
print("복원된 모델의 정확도: {:5.2f}%".format(100*acc))

# 체크포인트 콜백 함수는 몇 가지 매개변수를 제공. 체크포인트 이름을 고유하게 만들거나 체크포인트 주기를 조정할 수 있다.

# 새로운 모델을 훈련하고 다섯 번의 에포크마다 고유한 이름으로 체크포인트를 저장해 보자.

# 파일 이름에 에포크 번호를 포함시킨다(`str.format` 포맷)
checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True, period=5)  # 5 번째 에포크마다 가중치를 저장

model = create_model()
model.save_weights(checkpoint_path.format(epoch=0))
model.fit(train_images, train_labels, epochs = 50, callbacks = [cp_callback], validation_data = (test_images,test_labels), verbose=0)

# 만들어진 체크포인트를 확인해 보고 마지막 체크포인트를 선택
! ls {checkpoint_dir}

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

# 노트: 텐서플로는 기본적으로 최근 5개의 체크포인트만 저장
# 모델을 초기화하고 최근 체크포인트를 로드하여 테스트해 보자.
model = create_model()
model.load_weights(latest)
loss, acc = model.evaluate(test_images, test_labels)
print("복원된 모델의 정확도: {:5.2f}%".format(100*acc))

# 위 코드는 가중치를 일련의 체크포인트 포맷의 파일에 저장. 이 파일에 포함되는 것은 훈련된 이진 포맷의 가중치이다. 
# 체크포인트가 담고 있는 것은 다음과 같다:
## 모델의 가중치를 포함하는 하나 이상의 샤드(shard, 조각)
## 가중치가 어느 샤드에 저장되어 있는지를 나타내는 인덱스 파일
# 단일 머신에서 모델을 훈련한다면 .data-00000-of-00001 확장자를 가진 샤드 하나만 만들어 진다.

# 수동으로 가중치 저장하기   (앞에서는 가중치를 모델에 로드하는 방법을 설명)
# Model.save_weights 메서드를 사용

# 가중치를 저장
model.save_weights('./checkpoints/my_checkpoint')

# 가중치를 복원
model = create_model()
model.load_weights('./checkpoints/my_checkpoint')

loss,acc = model.evaluate(test_images, test_labels)
print("복원된 모델의 정확도: {:5.2f}%".format(100*acc))

# 모델 전체를 파일 하나에 저장할 수 있다. 가중치, 모델 구성 심지어 옵티마이저에 지정한 설정까지 저장된다. 
# 모델의 체크포인트를 저장하므로 원본 코드를 사용하지 않고 나중에 정확히 동일한 상태에서 훈련을 다시 시작할 수 있다.

# 전체 모델을 저장하는 기능은 매우 유용. TensorFlow.js로 모델을 로드한 다음 웹 브라우저에서 모델을 훈련하고 실행할 수 있다(HDF5, Saved Model). 
# 또는 모바일 장치에 맞도록 변환한 다음 TensorFlow Lite를 사용하여 실행할 수 있다(HDF5, Saved Model).

# HDF5 파일로 저장하기
# 케라스는 HDF5 표준을 따르는 기본 저장 포맷을 제공. 저장된 모델을 하나의 이진 파일(binary blob)처럼 다룰 수 있다.

model = create_model()
model.fit(train_images, train_labels, epochs=5)

# 전체 모델을 HDF5 파일로 저장합니다
model.save('my_model.h5')

# 이 파일로부터 모델을 다시 만들수 있다.
# 가중치와 옵티마이저를 포함하여 정확히 동일한 모델을 다시 생성한다.
new_model = keras.models.load_model('my_model.h5')
new_model.summary()

# 정확도를 확인
loss, acc = new_model.evaluate(test_images, test_labels)
print("복원된 모델의 정확도: {:5.2f}%".format(100*acc))

# 현재는 텐서플로 옵티마이저(tf.train)를 저장할 수 없다. 이런 경우에는 모델을 로드한 후에 다시 컴파일해야 한다. 옵티마이저의 상태는 유지되지 않는다.

